[10/09/2022 10:53:53 INFO]: 
_________________________________________________

[10/09/2022 10:53:53 INFO]: train_model.py main() running.
[10/09/2022 10:53:53 INFO]: problem:
  hyp:
    alpha: 0
    clip: 1
    epochs: 150
    lr: 0.001
    lr_decay: step
    lr_factor: 0.1
    lr_schedule:
    - 60
    - 100
    lr_throttle: false
    optimizer: adam
    save_period: -1
    test_batch_size: 500
    test_mode: default
    train_batch_size: 100
    train_mode: progressive
    val_period: 20
    warmup_period: 10
  model:
    model: feedforward_net_1d
    model_path: null
    width: 400
    max_iters: 30
    test_iterations:
      low: 30
      high: 30
  name: prefix_sums
  test_data: 512
  train_data: 32
train_log: train_log
name: prefix_sums_ablation
run_id: plushest-Jillana

2022-10-09 10:53:53.232581: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-10-09 10:53:53.362904: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-10-09 10:53:54.338106: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.6.0/lib64/:/local/java/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib64/
2022-10-09 10:53:54.338363: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.6.0/lib64/:/local/java/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib64/
2022-10-09 10:53:54.338371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Files already downloaded and verified
Loading data with 32 bits.
Files already downloaded and verified
Loading data with 512 bits.
[10/09/2022 10:53:57 INFO]: This feedforward_net_1d has 58.322 million parameters.
[10/09/2022 10:53:57 INFO]: Training will start at epoch 0.
[10/09/2022 10:53:57 INFO]: ==> Starting training for 150 epochs...
  0%|          | 0/80 [00:00<?, ?it/s]                                      Error executing job with overrides: ['problem.hyp.alpha=0', 'problem/model=ff_net_1d', 'problem=prefix_sums', 'name=prefix_sums_ablation', '+run_id=plushest-Jillana']
Traceback (most recent call last):
  File "/dcs/20/u2004277/Documents/deep-thinking/train_model.py", line 82, in main
    loss, acc = dt.train(net, loaders, cfg.problem.hyp.train_mode, train_setup, device)
  File "/dcs/20/u2004277/Documents/deep-thinking/deepthinking/utils/training.py", line 61, in train
    train_loss, acc = train_progressive(net, loaders, train_setup, device)
  File "/dcs/20/u2004277/Documents/deep-thinking/deepthinking/utils/training.py", line 121, in train_progressive
    loss.backward()
  File "/dcs/20/u2004277/.local/lib/python3.9/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/dcs/20/u2004277/.local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 1.27 GiB (GPU 0; 3.82 GiB total capacity; 463.97 MiB already allocated; 1.27 GiB free; 1.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
